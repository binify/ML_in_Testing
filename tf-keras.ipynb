{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train10K.csv')\n",
    "df = df[df['amount'] >0]\n",
    "\n",
    "train_set, test_set = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "y_train = train_set[[\"amount\"]].copy()\n",
    "X_train = train_set.drop([\"approved\",\"amount\",\"label\"],axis=1)\n",
    "X_train = X_train.values.astype(np.float32)\n",
    "y_train = y_train.values.reshape(-1)\n",
    "\n",
    "y_test = test_set[[\"amount\"]].copy()\n",
    "y_test = y_test.values.reshape(-1)\n",
    "X_test = test_set.drop([\"approved\",\"amount\",\"label\"],axis=1)\n",
    "X_test = X_test.values.astype(np.float32)\n",
    "\n",
    "X_valid, X_train = X_train[:1000], X_train[1000:]\n",
    "y_valid, y_train = y_train[:1000], y_train[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y = encoder.transform(y_train)\n",
    "dummy_y = np_utils.to_categorical(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12000.,   2900.,   2100., ...,   3600.,   7900.,   3900.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=8, kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(50,kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(100, input_dim=8, kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(50,kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(1,activation='linear',kernel_initializer='normal'))\n",
    "    \n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "estimator = KerasRegressor(build_fn=clf_model,epochs=50,batch_size=50,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, random_state=seed)\n",
    "result = cross_val_score(estimator,X_train,y_train,cv=kfold)\n",
    "scores = np.sqrt(-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_show(scores):\n",
    "    print(\"Score: \",scores)\n",
    "    print(\"Mean: \", scores.mean())\n",
    "    print(\"Standard Deviation: \", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  [-0.02920458 -0.00394147 -0.00739842 -0.00449282 -0.00421797]\n",
      "Mean:  -0.00985105163427\n",
      "Standard Deviation:  0.00975643138532\n"
     ]
    }
   ],
   "source": [
    "score_show(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3249 samples, validate on 813 samples\n",
      "Epoch 1/300\n",
      "3249/3249 [==============================] - 0s 75us/step - loss: 6560.2254 - val_loss: 6452.2011\n",
      "Epoch 2/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 6347.2644 - val_loss: 5959.5129\n",
      "Epoch 3/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 5385.0616 - val_loss: 4641.6412\n",
      "Epoch 4/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 4408.0104 - val_loss: 4169.8778\n",
      "Epoch 5/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 4170.5368 - val_loss: 4091.3231\n",
      "Epoch 6/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 4112.7357 - val_loss: 4081.3138\n",
      "Epoch 7/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 4097.4240 - val_loss: 4070.9023\n",
      "Epoch 8/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 4087.4843 - val_loss: 4060.9128\n",
      "Epoch 9/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 4077.7044 - val_loss: 4052.3514\n",
      "Epoch 10/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 4067.1013 - val_loss: 4041.0346\n",
      "Epoch 11/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 4057.5748 - val_loss: 4031.3394\n",
      "Epoch 12/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 4047.5130 - val_loss: 4022.7186\n",
      "Epoch 13/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 4038.0482 - val_loss: 4009.7896\n",
      "Epoch 14/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 4027.9078 - val_loss: 4000.6930\n",
      "Epoch 15/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 4018.3783 - val_loss: 3988.1738\n",
      "Epoch 16/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 4007.5480 - val_loss: 3980.4684\n",
      "Epoch 17/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3998.5324 - val_loss: 3970.3389\n",
      "Epoch 18/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3986.3581 - val_loss: 3954.3214\n",
      "Epoch 19/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3972.4510 - val_loss: 3943.0703\n",
      "Epoch 20/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3961.1096 - val_loss: 3928.1686\n",
      "Epoch 21/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3948.6458 - val_loss: 3919.7718\n",
      "Epoch 22/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 3933.3461 - val_loss: 3902.2027\n",
      "Epoch 23/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 3919.0566 - val_loss: 3884.2286\n",
      "Epoch 24/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 3905.0265 - val_loss: 3868.5190\n",
      "Epoch 25/300\n",
      "3249/3249 [==============================] - 0s 21us/step - loss: 3887.6146 - val_loss: 3854.1595\n",
      "Epoch 26/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3872.1704 - val_loss: 3835.9494\n",
      "Epoch 27/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3851.6401 - val_loss: 3815.2513\n",
      "Epoch 28/300\n",
      "3249/3249 [==============================] - 0s 13us/step - loss: 3833.4299 - val_loss: 3791.4548\n",
      "Epoch 29/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 3811.1273 - val_loss: 3776.5130\n",
      "Epoch 30/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 3785.9493 - val_loss: 3746.8522\n",
      "Epoch 31/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 3761.4116 - val_loss: 3726.0706\n",
      "Epoch 32/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3730.0575 - val_loss: 3685.9773\n",
      "Epoch 33/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3699.6319 - val_loss: 3653.3510\n",
      "Epoch 34/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3662.2795 - val_loss: 3620.8989\n",
      "Epoch 35/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3621.0576 - val_loss: 3567.7294\n",
      "Epoch 36/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 3576.9955 - val_loss: 3520.5047\n",
      "Epoch 37/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3522.1690 - val_loss: 3469.4902\n",
      "Epoch 38/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 3459.7278 - val_loss: 3401.7202\n",
      "Epoch 39/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 3389.9549 - val_loss: 3336.0244\n",
      "Epoch 40/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 3310.0281 - val_loss: 3246.5126\n",
      "Epoch 41/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 3212.7316 - val_loss: 3144.3000\n",
      "Epoch 42/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 3105.9206 - val_loss: 3045.3223\n",
      "Epoch 43/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 2988.0729 - val_loss: 2906.8853\n",
      "Epoch 44/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 2862.5313 - val_loss: 2774.0869\n",
      "Epoch 45/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 2721.9176 - val_loss: 2638.9601\n",
      "Epoch 46/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 2589.2576 - val_loss: 2516.3923\n",
      "Epoch 47/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 2454.3757 - val_loss: 2375.6558\n",
      "Epoch 48/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 2327.7886 - val_loss: 2229.1800\n",
      "Epoch 49/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 2198.9909 - val_loss: 2113.9714\n",
      "Epoch 50/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 2079.9219 - val_loss: 1976.1491\n",
      "Epoch 51/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 1964.6061 - val_loss: 1860.8150\n",
      "Epoch 52/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 1853.0457 - val_loss: 1752.2756\n",
      "Epoch 53/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 1754.5605 - val_loss: 1649.9726\n",
      "Epoch 54/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 1657.3191 - val_loss: 1551.6012\n",
      "Epoch 55/300\n",
      "3249/3249 [==============================] - 0s 21us/step - loss: 1574.7808 - val_loss: 1470.0565\n",
      "Epoch 56/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 1494.2643 - val_loss: 1383.7757\n",
      "Epoch 57/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 1425.6258 - val_loss: 1308.6624\n",
      "Epoch 58/300\n",
      "3249/3249 [==============================] - 0s 18us/step - loss: 1353.0145 - val_loss: 1236.1830\n",
      "Epoch 59/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 1291.1476 - val_loss: 1173.0043\n",
      "Epoch 60/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 1229.8304 - val_loss: 1126.7334\n",
      "Epoch 61/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 1172.4985 - val_loss: 1064.2396\n",
      "Epoch 62/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 1113.5976 - val_loss: 1012.5118\n",
      "Epoch 63/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 1062.1973 - val_loss: 969.0594\n",
      "Epoch 64/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 1011.7761 - val_loss: 922.1116\n",
      "Epoch 65/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 959.6879 - val_loss: 890.0211\n",
      "Epoch 66/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 914.3842 - val_loss: 833.8359\n",
      "Epoch 67/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 870.0506 - val_loss: 797.4290\n",
      "Epoch 68/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 830.1627 - val_loss: 792.2670\n",
      "Epoch 69/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 792.1727 - val_loss: 735.2129\n",
      "Epoch 70/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 756.7282 - val_loss: 748.5655\n",
      "Epoch 71/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 720.4250 - val_loss: 677.4876\n",
      "Epoch 72/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 693.2967 - val_loss: 650.3060\n",
      "Epoch 73/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 662.4504 - val_loss: 628.5021\n",
      "Epoch 74/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 637.4174 - val_loss: 607.2579\n",
      "Epoch 75/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 617.8035 - val_loss: 592.1282\n",
      "Epoch 76/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 595.0184 - val_loss: 576.9348\n",
      "Epoch 77/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 579.6523 - val_loss: 572.8432\n",
      "Epoch 78/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 561.4860 - val_loss: 538.0686\n",
      "Epoch 79/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 541.7759 - val_loss: 543.9303\n",
      "Epoch 80/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 528.1350 - val_loss: 518.5528\n",
      "Epoch 81/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 516.2902 - val_loss: 502.5605\n",
      "Epoch 82/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 506.4038 - val_loss: 492.1344\n",
      "Epoch 83/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 497.3081 - val_loss: 484.0999\n",
      "Epoch 84/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 487.9608 - val_loss: 475.7488\n",
      "Epoch 85/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 475.9606 - val_loss: 473.3428\n",
      "Epoch 86/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 469.8609 - val_loss: 462.6628\n",
      "Epoch 87/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 464.7576 - val_loss: 453.5334\n",
      "Epoch 88/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 457.3747 - val_loss: 450.5707\n",
      "Epoch 89/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 449.9525 - val_loss: 451.2587\n",
      "Epoch 90/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 443.6376 - val_loss: 437.4375\n",
      "Epoch 91/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 439.9057 - val_loss: 432.7676\n",
      "Epoch 92/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 440.1088 - val_loss: 446.7966\n",
      "Epoch 93/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 433.3566 - val_loss: 425.5348\n",
      "Epoch 94/300\n",
      "3249/3249 [==============================] - 0s 18us/step - loss: 431.6586 - val_loss: 428.6266\n",
      "Epoch 95/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 422.2202 - val_loss: 418.2700\n",
      "Epoch 96/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 430.0203 - val_loss: 422.3468\n",
      "Epoch 97/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 415.1475 - val_loss: 412.1435\n",
      "Epoch 98/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 411.0883 - val_loss: 406.6524\n",
      "Epoch 99/300\n",
      "3249/3249 [==============================] - 0s 18us/step - loss: 404.4935 - val_loss: 405.7001\n",
      "Epoch 100/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 401.9735 - val_loss: 400.7512\n",
      "Epoch 101/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 399.2670 - val_loss: 394.7150\n",
      "Epoch 102/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 397.7977 - val_loss: 407.8457\n",
      "Epoch 103/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 393.2556 - val_loss: 388.9809\n",
      "Epoch 104/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 391.0935 - val_loss: 390.4540\n",
      "Epoch 105/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 383.1808 - val_loss: 393.9467\n",
      "Epoch 106/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 390.8364 - val_loss: 387.8716\n",
      "Epoch 107/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 379.7779 - val_loss: 394.2281\n",
      "Epoch 108/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 379.0352 - val_loss: 380.7813\n",
      "Epoch 109/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 372.5351 - val_loss: 379.3408\n",
      "Epoch 110/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 369.8468 - val_loss: 379.7633\n",
      "Epoch 111/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 373.5241 - val_loss: 374.0774\n",
      "Epoch 112/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 367.3987 - val_loss: 369.1175\n",
      "Epoch 113/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 363.1902 - val_loss: 366.6956\n",
      "Epoch 114/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 359.6758 - val_loss: 363.4975\n",
      "Epoch 115/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 355.4837 - val_loss: 381.1258\n",
      "Epoch 116/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 358.0135 - val_loss: 364.3284\n",
      "Epoch 117/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 355.0021 - val_loss: 363.8105\n",
      "Epoch 118/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 349.9655 - val_loss: 356.4086\n",
      "Epoch 119/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 347.0202 - val_loss: 358.1506\n",
      "Epoch 120/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 343.1779 - val_loss: 354.3308\n",
      "Epoch 121/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 341.1571 - val_loss: 348.3902\n",
      "Epoch 122/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 341.2877 - val_loss: 348.7546\n",
      "Epoch 123/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 337.9239 - val_loss: 350.6057\n",
      "Epoch 124/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 336.9797 - val_loss: 347.5568\n",
      "Epoch 125/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 336.4735 - val_loss: 340.8550\n",
      "Epoch 126/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 331.6176 - val_loss: 336.8124\n",
      "Epoch 127/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 328.2451 - val_loss: 332.6994\n",
      "Epoch 128/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 323.6988 - val_loss: 336.8840\n",
      "Epoch 129/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 322.8721 - val_loss: 328.5017\n",
      "Epoch 130/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 321.3174 - val_loss: 323.1331\n",
      "Epoch 131/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 317.7626 - val_loss: 328.4402\n",
      "Epoch 132/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 314.1319 - val_loss: 321.6208\n",
      "Epoch 133/300\n",
      "3249/3249 [==============================] - 0s 20us/step - loss: 312.3296 - val_loss: 317.6317\n",
      "Epoch 134/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 312.9227 - val_loss: 327.0225\n",
      "Epoch 135/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 318.9123 - val_loss: 341.7694\n",
      "Epoch 136/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 312.3269 - val_loss: 313.4042\n",
      "Epoch 137/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 308.5490 - val_loss: 316.3008\n",
      "Epoch 138/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 307.5141 - val_loss: 336.9502\n",
      "Epoch 139/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 304.8729 - val_loss: 312.1862\n",
      "Epoch 140/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 302.0146 - val_loss: 315.7143\n",
      "Epoch 141/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 302.8103 - val_loss: 312.9274\n",
      "Epoch 142/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 296.8486 - val_loss: 305.4791\n",
      "Epoch 143/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 295.9287 - val_loss: 300.1183\n",
      "Epoch 144/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 292.9988 - val_loss: 303.6560\n",
      "Epoch 145/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 292.2263 - val_loss: 297.1877\n",
      "Epoch 146/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 293.8357 - val_loss: 299.0837\n",
      "Epoch 147/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 289.7606 - val_loss: 296.3040\n",
      "Epoch 148/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 292.9352 - val_loss: 304.2160\n",
      "Epoch 149/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 284.9742 - val_loss: 291.3945\n",
      "Epoch 150/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 282.5923 - val_loss: 290.3987\n",
      "Epoch 151/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 281.2149 - val_loss: 283.6054\n",
      "Epoch 152/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 278.8000 - val_loss: 282.4699\n",
      "Epoch 153/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 277.1623 - val_loss: 289.6572\n",
      "Epoch 154/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 275.8540 - val_loss: 283.3850\n",
      "Epoch 155/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 275.3731 - val_loss: 276.4002\n",
      "Epoch 156/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 271.5394 - val_loss: 274.1288\n",
      "Epoch 157/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 269.4880 - val_loss: 278.3504\n",
      "Epoch 158/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 267.7262 - val_loss: 272.5625\n",
      "Epoch 159/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 269.3479 - val_loss: 269.9344\n",
      "Epoch 160/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 266.8542 - val_loss: 280.2577\n",
      "Epoch 161/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 264.2541 - val_loss: 268.4327\n",
      "Epoch 162/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 264.0677 - val_loss: 268.4222\n",
      "Epoch 163/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 262.9947 - val_loss: 261.8828\n",
      "Epoch 164/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 262.3725 - val_loss: 262.8999\n",
      "Epoch 165/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 259.8200 - val_loss: 260.0333\n",
      "Epoch 166/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 258.6390 - val_loss: 259.6809\n",
      "Epoch 167/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 260.0668 - val_loss: 261.5597\n",
      "Epoch 168/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 259.0382 - val_loss: 269.4995\n",
      "Epoch 169/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 258.2053 - val_loss: 260.8753\n",
      "Epoch 170/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 253.1181 - val_loss: 259.4757\n",
      "Epoch 171/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 252.1215 - val_loss: 256.5340\n",
      "Epoch 172/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 248.7281 - val_loss: 253.2183\n",
      "Epoch 173/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 248.5337 - val_loss: 248.0439\n",
      "Epoch 174/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 247.7034 - val_loss: 248.3556\n",
      "Epoch 175/300\n",
      "3249/3249 [==============================] - 0s 20us/step - loss: 248.0592 - val_loss: 261.3151\n",
      "Epoch 176/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 245.3552 - val_loss: 245.0622\n",
      "Epoch 177/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 242.1277 - val_loss: 247.1062\n",
      "Epoch 178/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 241.4296 - val_loss: 248.5887\n",
      "Epoch 179/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 241.5405 - val_loss: 241.9028\n",
      "Epoch 180/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 238.4439 - val_loss: 237.5375\n",
      "Epoch 181/300\n",
      "3249/3249 [==============================] - 0s 21us/step - loss: 236.7090 - val_loss: 238.6777\n",
      "Epoch 182/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 235.6841 - val_loss: 239.1985\n",
      "Epoch 183/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 234.9958 - val_loss: 240.5079\n",
      "Epoch 184/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 233.1913 - val_loss: 233.7711\n",
      "Epoch 185/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 236.3990 - val_loss: 237.3610\n",
      "Epoch 186/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 233.3843 - val_loss: 257.2437\n",
      "Epoch 187/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 228.0438 - val_loss: 227.6685\n",
      "Epoch 188/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 229.5774 - val_loss: 226.5763\n",
      "Epoch 189/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 228.1154 - val_loss: 237.9617\n",
      "Epoch 190/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 227.2267 - val_loss: 224.6536\n",
      "Epoch 191/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 228.1484 - val_loss: 241.4823\n",
      "Epoch 192/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 222.2640 - val_loss: 224.6634\n",
      "Epoch 193/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 222.8735 - val_loss: 224.1808\n",
      "Epoch 194/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 218.8402 - val_loss: 225.1280\n",
      "Epoch 195/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 220.5215 - val_loss: 226.2237\n",
      "Epoch 196/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 217.1518 - val_loss: 217.2857\n",
      "Epoch 197/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 216.8985 - val_loss: 224.2119\n",
      "Epoch 198/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 218.6420 - val_loss: 218.0433\n",
      "Epoch 199/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 212.9238 - val_loss: 214.4056\n",
      "Epoch 200/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 219.1236 - val_loss: 213.9753\n",
      "Epoch 201/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 212.8360 - val_loss: 210.8172\n",
      "Epoch 202/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 214.6502 - val_loss: 212.3692\n",
      "Epoch 203/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 210.5301 - val_loss: 211.1206\n",
      "Epoch 204/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 208.0093 - val_loss: 209.0997\n",
      "Epoch 205/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 207.4289 - val_loss: 204.9837\n",
      "Epoch 206/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 206.9379 - val_loss: 209.2476\n",
      "Epoch 207/300\n",
      "3249/3249 [==============================] - 0s 26us/step - loss: 209.6413 - val_loss: 216.6542\n",
      "Epoch 208/300\n",
      "3249/3249 [==============================] - 0s 25us/step - loss: 210.0659 - val_loss: 204.0596\n",
      "Epoch 209/300\n",
      "3249/3249 [==============================] - 0s 21us/step - loss: 203.3875 - val_loss: 218.7337\n",
      "Epoch 210/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 208.3643 - val_loss: 212.4939\n",
      "Epoch 211/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 202.4391 - val_loss: 202.3605\n",
      "Epoch 212/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 200.3601 - val_loss: 198.9013\n",
      "Epoch 213/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 201.4754 - val_loss: 200.8309\n",
      "Epoch 214/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 198.5287 - val_loss: 198.8698\n",
      "Epoch 215/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 196.9382 - val_loss: 195.3142\n",
      "Epoch 216/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 200.9456 - val_loss: 196.9274\n",
      "Epoch 217/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 195.2519 - val_loss: 193.9833\n",
      "Epoch 218/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 194.4960 - val_loss: 210.0890\n",
      "Epoch 219/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 195.1468 - val_loss: 200.4006\n",
      "Epoch 220/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 195.7426 - val_loss: 190.5063\n",
      "Epoch 221/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 194.2758 - val_loss: 198.1288\n",
      "Epoch 222/300\n",
      "3249/3249 [==============================] - 0s 18us/step - loss: 191.0481 - val_loss: 189.5937\n",
      "Epoch 223/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 193.9792 - val_loss: 194.5259\n",
      "Epoch 224/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 189.1958 - val_loss: 190.2044\n",
      "Epoch 225/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 188.7934 - val_loss: 186.2369\n",
      "Epoch 226/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 187.4897 - val_loss: 189.4094\n",
      "Epoch 227/300\n",
      "3249/3249 [==============================] - 0s 20us/step - loss: 186.2152 - val_loss: 184.0943\n",
      "Epoch 228/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 186.3161 - val_loss: 190.2155\n",
      "Epoch 229/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 186.2737 - val_loss: 188.1836\n",
      "Epoch 230/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 191.2683 - val_loss: 209.6812\n",
      "Epoch 231/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 187.0920 - val_loss: 183.6663\n",
      "Epoch 232/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 184.8780 - val_loss: 180.6899\n",
      "Epoch 233/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 183.5600 - val_loss: 180.6927\n",
      "Epoch 234/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 183.8702 - val_loss: 181.7747\n",
      "Epoch 235/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 182.3142 - val_loss: 184.6228\n",
      "Epoch 236/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 184.5815 - val_loss: 183.8370\n",
      "Epoch 237/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 182.3243 - val_loss: 183.0854\n",
      "Epoch 238/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 180.6563 - val_loss: 186.6763\n",
      "Epoch 239/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 180.9490 - val_loss: 176.8456\n",
      "Epoch 240/300\n",
      "3249/3249 [==============================] - 0s 18us/step - loss: 181.1112 - val_loss: 178.6274\n",
      "Epoch 241/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 181.2497 - val_loss: 175.3695\n",
      "Epoch 242/300\n",
      "3249/3249 [==============================] - 0s 18us/step - loss: 181.3033 - val_loss: 180.3100\n",
      "Epoch 243/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 179.9807 - val_loss: 178.5695\n",
      "Epoch 244/300\n",
      "3249/3249 [==============================] - 0s 20us/step - loss: 181.2454 - val_loss: 175.7420\n",
      "Epoch 245/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 186.1464 - val_loss: 176.1167\n",
      "Epoch 246/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 183.3372 - val_loss: 172.8835\n",
      "Epoch 247/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 176.5747 - val_loss: 177.6722\n",
      "Epoch 248/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 178.4129 - val_loss: 198.0721\n",
      "Epoch 249/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 176.4736 - val_loss: 179.6999\n",
      "Epoch 250/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 176.4464 - val_loss: 168.9447\n",
      "Epoch 251/300\n",
      "3249/3249 [==============================] - 0s 20us/step - loss: 174.7171 - val_loss: 174.0680\n",
      "Epoch 252/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 173.8042 - val_loss: 173.7473\n",
      "Epoch 253/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 173.0743 - val_loss: 169.3780\n",
      "Epoch 254/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 174.6571 - val_loss: 175.2204\n",
      "Epoch 255/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 175.1465 - val_loss: 176.6870\n",
      "Epoch 256/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 177.3496 - val_loss: 168.5231\n",
      "Epoch 257/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 173.1073 - val_loss: 168.1027\n",
      "Epoch 258/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 173.3847 - val_loss: 168.1963\n",
      "Epoch 259/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 175.1244 - val_loss: 165.8596\n",
      "Epoch 260/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 171.6442 - val_loss: 167.6065\n",
      "Epoch 261/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 171.2266 - val_loss: 169.0454\n",
      "Epoch 262/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 171.5974 - val_loss: 170.1761\n",
      "Epoch 263/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 172.9470 - val_loss: 170.9941\n",
      "Epoch 264/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 172.5113 - val_loss: 163.0979\n",
      "Epoch 265/300\n",
      "3249/3249 [==============================] - 0s 18us/step - loss: 170.8428 - val_loss: 163.4959\n",
      "Epoch 266/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 170.2162 - val_loss: 181.8403\n",
      "Epoch 267/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 170.3566 - val_loss: 165.0433\n",
      "Epoch 268/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 167.7386 - val_loss: 160.9591\n",
      "Epoch 269/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 169.8579 - val_loss: 162.0637\n",
      "Epoch 270/300\n",
      "3249/3249 [==============================] - 0s 20us/step - loss: 168.7167 - val_loss: 160.0963\n",
      "Epoch 271/300\n",
      "3249/3249 [==============================] - 0s 20us/step - loss: 167.5331 - val_loss: 162.4583\n",
      "Epoch 272/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 168.6531 - val_loss: 163.8608\n",
      "Epoch 273/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 166.5728 - val_loss: 159.6898\n",
      "Epoch 274/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 166.2122 - val_loss: 172.0772\n",
      "Epoch 275/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 167.7186 - val_loss: 159.5341\n",
      "Epoch 276/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 165.9931 - val_loss: 159.1709\n",
      "Epoch 277/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 168.2295 - val_loss: 159.9748\n",
      "Epoch 278/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 167.0142 - val_loss: 178.5004\n",
      "Epoch 279/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 166.7680 - val_loss: 156.2902\n",
      "Epoch 280/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 169.5401 - val_loss: 164.3873\n",
      "Epoch 281/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 166.0708 - val_loss: 157.0175\n",
      "Epoch 282/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 165.3424 - val_loss: 157.1469\n",
      "Epoch 283/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 164.1756 - val_loss: 154.5540\n",
      "Epoch 284/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 164.5010 - val_loss: 156.5963\n",
      "Epoch 285/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 164.5082 - val_loss: 153.8622\n",
      "Epoch 286/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 161.2911 - val_loss: 156.1695\n",
      "Epoch 287/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 164.5208 - val_loss: 171.2946\n",
      "Epoch 288/300\n",
      "3249/3249 [==============================] - 0s 15us/step - loss: 163.4433 - val_loss: 154.0587\n",
      "Epoch 289/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 166.3077 - val_loss: 162.6257\n",
      "Epoch 290/300\n",
      "3249/3249 [==============================] - 0s 19us/step - loss: 162.3834 - val_loss: 169.1136\n",
      "Epoch 291/300\n",
      "3249/3249 [==============================] - 0s 21us/step - loss: 163.8559 - val_loss: 163.5382\n",
      "Epoch 292/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 162.0837 - val_loss: 157.3473\n",
      "Epoch 293/300\n",
      "3249/3249 [==============================] - 0s 14us/step - loss: 160.2364 - val_loss: 152.2479\n",
      "Epoch 294/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 159.8219 - val_loss: 150.1017\n",
      "Epoch 295/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 159.4471 - val_loss: 154.7777\n",
      "Epoch 296/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 160.7230 - val_loss: 151.9002\n",
      "Epoch 297/300\n",
      "3249/3249 [==============================] - 0s 17us/step - loss: 161.5157 - val_loss: 150.1717\n",
      "Epoch 298/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 160.6668 - val_loss: 155.0646\n",
      "Epoch 299/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 160.4707 - val_loss: 148.2105\n",
      "Epoch 300/300\n",
      "3249/3249 [==============================] - 0s 16us/step - loss: 159.7871 - val_loss: 149.1628\n"
     ]
    }
   ],
   "source": [
    "model = regression_model()\n",
    "history = model.fit(X_train, y_train, epochs=300,batch_size=50,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAEjCAYAAADpH9ynAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8VPW5+PHPMzPZE0iAsC8BRQUUESNCResuait4tRWvtbS1l1/V1qWr7e11q7ba21pra7VqVWxdi/VqWzdU0GpFCYrInrCHNRCW7Mlknt8f5xsY4mRllkzyvF+vec2Z7zlzznPWZ873fOccUVWMMcaYePAlOgBjjDE9hyUdY4wxcWNJxxhjTNxY0jHGGBM3lnSMMcbEjSUdY4wxcdPtk46IXC0iO0SkUkT6JjqeaBORW0XkL4mO43CJSIGIqIgEojCuV0RkVjTiijBuFZEjO/ndK0Tk9WjHFC8iskFEznbdPxGRRxIdU7xFczvtxLS/JiLvhn2uFJFRMZjOchE5PdrjbRK3pCMi/ykiRW5BbXMHhqmHOc4DO0EL/VOAe4BzVTVbVXcfzvTCprlDRLLCyr4pIgsOd9zRJiKnux3k/mbl74rI19o5jk4fZGPJHfTWu+2pVESebeqnquer6pwEx/eZg5OqPqmq58ZwmlHfx1qiqj9X1W+66bb7QOwOnCoiX45FXF2FiCwQkT0ikharabhj2rrDGYeIPC4idzQb7zhVXXBYwbUiLklHRL4L3Av8HBgADAf+AEyP8aQHAOnA8o5+UTwtLZ8AcP3hBNbOGKLxa6oK+KqIFERhXDHR0fl0ZzFXAmerajZQCLwZi9iSRUf3sUT8UndmAeXuPSYSOG9N0y8ATgUUuCiRsXRJqhrTF9AbqAS+1MowaXg7zFb3uhdIc/36Af8A9uJtrP/CS5Z/BkJAjRv/D5uN8yi8A666/m+58s8Bi4B97v1zYd9ZANwJvOfGe2SEWDcAN7lYcl3ZN4EFYcMcA8xzw6wGvtxsGt8M+/w14N2wzwpcCxQD613Zb4HNwH5gMXBq2PC3An9pYbmeDpQCvwMeCyt/F/ha2OdvACuBPcBrwAhX/o6Lp8otw8uAt4FLXP+prv8F7vPZwBLX7QN+CmwEdgJPAL1dvwL3vauATW46TWUBN8wlblkfG2G+fg/c28r2dGAZu+X7HvAbvG1ondsGvuaW6U5gVgfXz5Gu+0LgY7deNgO3hg23iYPbXiUwpYVxfcut6z3A/YC4fn7g18AuYD3w7fDl04l97FZgLvAXF+833Tq6CVgL7AaeA/qEfedKt/52A//t1sfZzbe7SPPaQgwj8PbZS4AgMCDCtvoTN88bgCvC+j8OPIi3X1XgbYcj2thvIu7rwEygqFlsNwIvtWO9FrS0HsKGuRlvm7sH+Eezfu2Zj+vwttNdwP8CvnZsixlue9no5vddIMP1+yuw3ZW/A4xz5bOBBqDerbe/hx3jmtZza8fmpnX2Pbz9aBvw9ZaWy4G42xrgcF/ANLeBtbaSbgcWAv2BfODfwM9cv1+4lZTiXqdycMc8sHBaGO8hGwjQB2/nvhLvbOVy97lv2AFnEzDO9U+JMM4NeAfXvwF3uLIDSQfIchvq1904JrqNZ1zYNNo6qM1zsTZtNF8B+rrxfc9tQOnNd/4IsTZtFAPxdqCjXfmBpAPMAEqAMW78PwX+HWnDDltXv3PdP8E7YN0d1u+3rvsbbryjgGy3vP7cbL084ZZXRvi6csuuhAhJP2x5lAM/wDvL8Tfrf2AZu+UbdOP0A3e4dXw/3g51Lt7On92B9XNk2PI9Du/gPR7YAcxo6eDUwrj+AeTinZmUAdNcv28BK4ChQB7wRvPxdXAfuxXvADPDxZsB3IC33w11y+KPwNNu+LF4B6LTXL973DQiJZ3PzGsLMfwP8KHr/hT4brNtNeimkwZ8Hu/HTtM2+7hbT03x/DbCsjyw39DKvg5kunGNDvv+ImBmZ9ZrhPksAa4BTnTLPDy5tmc+5rv4hwNrOHRbbmlbvB9v2x2Ct51/joPJ4RtADgcTyJJm8dwR6RjXjmNz0zq7He/YfAFQDeS1uh201jMaL+AKYHsbw6zF/Vp2n88DNoTN9Iu0fNbRkaRzJW6jDxvmfQ4egBcAt7cR6wa8pHMs3i+HfA5NOpcB/2r2nT8Ct4RNo62D2pltxLAHOL75zh9huNOBUtf9S+BZ1x2edF4Brgr7js9tOCOab9ju81nAUtf9qpv3he7z28B/uO43gWvCvnc03g4YCFsvoyKsq+/jDrbt2K7ewDsw7QZuCut3YBm75Vsc1u84N53wA8FuYEIH1k9LyfBe4DeRtr1WxjU17PNzTfMBvAX8v7B+ZzcfXwf3sVuBd5qVrQTOCvs8KGwd3Qw8E9YvC+8X8eEknWLgBtf9Y+CTZttqEMhqtjz+x3U/3iyebKARGBZpv6Htff0vwM2uezReIsjszHptNuxUtwz7uc+rgBvD+rdnPqaF9b8GeLO1bRFvn63BHRPaWAe57nu9w+JpLem0dmw+3U03fBvfCUxuLYZ4XNPZDfRro551MN5pYZONrgy808sS4HURWSciNx1GLM2n0zStIWGfN7dnRKq6DO9XavN4RgAni8jephfeQWFgB+I8JAYR+Z6IrBSRfW58vfGqHTvibuA8ETk+Qry/DYu1HBAOXSbh3geOEpEBwAS8s5VhItIPmIR3+g6R12kA73pDxPl0fgDcr6qlrc2Mehflz8bbib4F3C4i57Uw+I6w7hr3/eZl2a1NLxIROVlE5otImYjsc3F0dL1sD+uuDotjMIcun9a2y/bsY5HGMQJ4IWzdr8Q7AA5oPn1VbUrunSIipwAjgWdc0VPAcSIyIWywPW46TcKPA4fEr6qVeNtqxP60va8/hXf2A/CfwP+parWL9XDW6yzgdVXdFTadWc2G6ch8NF8GkfTDu3a9tnkPEfGLyF0islZE9uMllKbvtEdrx2aA3aoaDPscvg1HFI+k8z5Qi3da35KteDtAk+GuDFWtUNXvqeoo4IvAd0XkLDecdjCW5tNpmtaWsM8dGectwH/x2aT1tqrmhr2yVfVq178K7/S+SaRkdCAGETkV+BHwZbzT1ly8MyzpQJyo13LvXuBnzXptxvtFHR5vhqr+u4XxVONdV7oeWKaq9Xin3N8F1obtbJHWaZBDE0CkZX0u8FMRuaSd89Wgqn8FluKdfR6u9qyfJk8BL+H9Su2NVw3ctF46um02tw2v2qvJsFaGbc8+FimmzcD5zdZ9uqpucdM/ME0RycSrmmrPeCOZhbdslojIduADV/7VsGHywluFEnYccMLjycarggrvHx5HW/v663iJegJe8nkqbLjW1muLRCQDbz/9vIhsd/N5I3B8sx97bc1H+Lpuvgwi2YW3/o+I0O8/8RqTnI33Y7WgadLuva111+KxubNinnRUdR/eqfr9IjJDRDJFJEVEzheRX7rBnsY70OS7X8w3453+IiJfEJEjRUTwrks0uhd4B7COtFN/Ge9X+n+KSEBELsOru/5HJ+etBHgW78Jfk3+4aVzp5jNFRE4SkTGu/xLgP9xyOBLvYnprcvAO1mVAQERuBnp1Jl68+vLP4V2/afIg8GMRGQcgIr1F5Eth/SMt47fxLmy/7T4vaPYZvHV6o4iMdDvWz/Gq98J/FUWyHO8axf0iErHlj2t2e6GI5IiIT0TOx7sO90Gk4TuoI+snByhX1VoRmYS3gzcpw7to3tn/UTwHXC8iQ0QkF++HR0Tt3McieRC4U0RGALj9r6m121zgCyIyVURS8aq5WzpetDqvIpKOdzCejXd23PT6DnBFszO020Qk1f3Y+gLeRfAmF4TF8zPgA1Vt6Qyw1X3dbYdz8WpS+uBdD2rS2nptzQy8Y9PYsHkcg9f4KTy5tjUfPxCRPBEZhvfj7llaoaoh4FHgHhEZ7M5upojXXDsHqMM7S83E2w/DtXUMbfHY3FlxaTKtqvfg/RL+Kd4GuhnvIPV/bpA7gCK8X6ufAh+5MvDqW9/Au6j5PvAHPdiG/Bd4C2SviHy/HXHsxtuQv4e3En4IfCHs13ln3I5X3900jQq8X+sz8X4RbMer2mpqr/8bvLrxHcAc4Mk2xv8a3nWXNXintrW0swqwOVXdj3dtp09Y2Qsuvmfc6fcy4Pywr90KzHHLuOm/FW/jbczvtPAZvJ3gz65svYv7O+2M8xO89fSwSyjN7cdrxLAJr0XaL4GrVfXdCMN2VEfWzzV41XoVeDvjc2HzUI1rCemW3eQOxvEw3q/xpXgtqV7G+/HRGGngduxjkfwW7xf9624eFgInu/Etx2sN9hTeWc8evEYpkabd1rzOwKvCfEJVtze9gD/hXfSe5obb7qazFW+5f0tVV4WN5ym82oVyvIv0V7Q0Y+3c15/COwP4a7MfQy2u1zbMwmsluqnZfP6eQ5NrW/PxIl5twhLgn3jLqS3fxzt2LnLjvRvv+P4E3nFjC9610oXNvvcnYKxbb5G2ldaOzZ3S1ArMGNOFueT7oKo2rzLqFsT7B/xfVHVoC/0fx2sU89N4xhVtbc2HiCheq7qSuAYWR93+NjjGJCMRyRCRC1zV0BC8X8YvJDouYw6XJR1juiYBbsOrbvoYr2XZzQmNyJgosOo1Y4wxcWNnOsYYY+LGko4xxpi4saRjjDEmbizpGGOMiRtLOsYYY+LGko4xxpi4saRjjDEmbizpGGOMiRtLOsYYY+LGko4xxpi4saRjjDEmbizpGGOMiRtLOsYYY+LGko4xxpi4CbQ9SPLp16+fFhQUJDoMY4xJKosXL96lqvmxnEa3TDoFBQUUFRUlOgxjjEkqIrIx1tOw6jVjjDFxY0nHGGNM3FjSMcYYEzfd8pqOMabnaWhooLS0lNra2kSH0uWlp6czdOhQUlJS4j5tSzrGmG6htLSUnJwcCgoKEJFEh9NlqSq7d++mtLSUkSNHxn36Vr1mjOkWamtr6du3ryWcNogIffv2TdgZoSUdY0y3YQmnfRK5nCzphNmyt4Zfv76aTburEx2KMcZ0S5Z0wuyvaeB3b5XwSeneRIdijEkye/fu5Q9/+EOHv3fBBRewd2/POeZY0gkzsl8WIrC2rDLRoRhjkkxLSaexsbHV77388svk5ubGKqwux1qvhUmnns/33sHW7b2BoxIdjjEmidx0002sXbuWCRMmkJKSQnZ2NoMGDWLJkiWsWLGCGTNmsHnzZmpra7n++uuZPXs2cPC2XZWVlZx//vlMnTqVf//73wwZMoQXX3yRjIyMBM9ZdFnSCbdjOY/X3sgt228BpiY6GmNMJ9329+Ws2Lo/quMcO7gXt3xxXIv977rrLpYtW8aSJUtYsGABF154IcuWLTvQLPnRRx+lT58+1NTUcNJJJ3HJJZfQt2/fQ8ZRXFzM008/zcMPP8yXv/xlnn/+eb7yla9EdT4SzZJOuHTvFLd6/25CIcXns5YwxpjOmTRp0iH/g7nvvvt44YUXANi8eTPFxcWfSTojR45kwoQJAJx44ols2LAhbvHGiyWdcBle0slsrGD7/loG53av01pjeorWzkjiJSsr60D3ggULeOONN3j//ffJzMzk9NNPj/g/mbS0tAPdfr+fmpqauMQaT9aQIFx6bwB6U2WNCYwxHZKTk0NFRUXEfvv27SMvL4/MzExWrVrFwoUL4xxd12FnOuH8KYRSsugdrGJfTUOiozHGJJG+fftyyimncOyxx5KRkcGAAQMO9Js2bRoPPvgg48eP5+ijj2by5MkJjDSxLOk0E0rPpXdtFbUNoUSHYoxJMk899VTE8rS0NF555ZWI/Zqu2/Tr149ly5YdKP/+978f9fi6Aqtea0bTc+lNFXXB1tvWG2OM6ThLOs1Iei69xM50jDEmFizpNCOZefSmitoGO9Mxxphos6TTjC8jl95SRZ0lHWOMiTpLOs1IRi65VFIXtOo1Y4yJtpgmHRHJFZG5IrJKRFaKyBQR6SMi80Sk2L3nuWFFRO4TkRIRWSoiE8PGM8sNXywis2IZMxm5ZEg9DXXd709ZxhiTaLE+0/kt8KqqHgMcD6wEbgLeVNXRwJvuM8D5wGj3mg08ACAifYBbgJOBScAtTYkqJjK8UUvtvphNwhhjsrOzAdi6dSuXXnppxGFOP/10ioqKWh3PvffeS3V18jwDLGZJR0R6AacBfwJQ1XpV3QtMB+a4weYAM1z3dOAJ9SwEckVkEHAeME9Vy1V1DzAPmBaruJvuvxaot6RjjIm9wYMHM3fu3E5/35LOQaOAMuAxEflYRB4RkSxggKpuA3Dv/d3wQ4DNYd8vdWUtlceGu/+a35KOMaYDfvSjHx3yPJ1bb72V2267jbPOOouJEydy3HHH8eKLL37mexs2bODYY48FoKamhpkzZzJ+/Hguu+yyQ+69dvXVV1NYWMi4ceO45ZZbAO8molu3buWMM87gjDPOAOD1119nypQpTJw4kS996UtUVnatW3rF8o4EAWAi8B1V/UBEfsvBqrRIIt3SWVspP/TLIrPxquUYPnx4x6Ntku5Vr6XUR/e26MaYOHrlJtj+aXTHOfA4OP+uFnvPnDmTG264gWuuuQaA5557jldffZUbb7yRXr16sWvXLiZPnsxFF12ESOQ72D/wwANkZmaydOlSli5dysSJBy5tc+edd9KnTx8aGxs566yzWLp0Kddddx333HMP8+fPp1+/fuzatYs77riDN954g6ysLO6++27uuecebr755ugui8MQy6RTCpSq6gfu81y8pLNDRAap6jZXfbYzbPhhYd8fCmx15ac3K1/QfGKq+hDwEEBhYeFnklK7pbg7SwetIYExpv1OOOEEdu7cydatWykrKyMvL49BgwZx44038s477+Dz+diyZQs7duxg4MCBEcfxzjvvcN111wEwfvx4xo8ff6Dfc889x0MPPUQwGGTbtm2sWLHikP4ACxcuZMWKFZxyyikA1NfXM2XKlBjNcefELOmo6nYR2SwiR6vqauAsYIV7zQLucu9N55svAd8WkWfwGg3sc4npNeDnYY0HzgV+HKu48ad68TfWx2wSxpgYa+WMJJYuvfRS5s6dy/bt25k5cyZPPvkkZWVlLF68mJSUFAoKCiI+0iBcpLOg9evX86tf/YpFixaRl5fH1772tYjjUVXOOeccnn766ajNU7TFuvXad4AnRWQpMAH4OV6yOUdEioFz3GeAl4F1QAnwMHANgKqWAz8DFrnX7a4sNgJe0iFoSccY0zEzZ87kmWeeYe7cuVx66aXs27eP/v37k5KSwvz589m4cWOr3z/ttNN48sknAVi2bBlLly4FYP/+/WRlZdG7d2927NhxyM1Dwx+pMHnyZN577z1KSkoAqK6uZs2aNbGY1U6L6V2mVXUJUBih11kRhlXg2hbG8yjwaHSja4E708HOdIwxHTRu3DgqKioYMmQIgwYN4oorruCLX/wihYWFTJgwgWOOOabV71999dV8/etfZ/z48UyYMIFJkyYBcPzxx3PCCScwbtw4Ro0adaD6DGD27Nmcf/75DBo0iPnz5/P4449z+eWXU1dXB8Add9zBUUcdFbuZ7iDxjvXdS2FhobbVtr1F1eXwy5H8JvBNbvzpr6MbmDEmZlauXMmYMWMSHUbSiLS8RGSxqkY6UYgauw1Oc+5MRxrrEhyIMcZ0P5Z0mjtQvWZPDjXGmGizpNOcP8V7U7umY0yy6Y6XC2IhkcvJkk5zIjRKCn4NEmy0O00bkyzS09PZvXu3JZ42qCq7d+8mPT09IdOPaeu1ZNXoSyGFILXBENl+y8vGJIOhQ4dSWlpKWVlZokPp8tLT0xk6dGhCpm1JJ4KQL5VUGqhtaCQ7zRaRMckgJSWFkSNHJjoM0wb7GR+BujMde5CbMcZElyWdCEL+VNIkSK09stoYY6LKkk4kTdd0LOkYY0xUWdKJQP2ppFr1mjHGRJ0lnUj8qXamY4wxMWBJJ5KAl3TqGuxMxxhjosmSTgTiTyNNGqgL2pmOMcZEkyWdCCTQVL1mZzrGGBNNlnQi8AW8hgR2TccYY6LLkk4EvpQ0a0hgjDExYEknAl8gjVQarMm0McZEmSWdCHwpaaRIo13TMcaYKLOkE8GBazrWes0YY6LKkk4k/lTSaLD/6RhjTJRZ0onEn+pVr9mZjjHGRFVMk46IbBCRT0VkiYgUubI+IjJPRIrde54rFxG5T0RKRGSpiEwMG88sN3yxiMyKZcwA+A8+T8cYY0z0xONM5wxVnaCqhe7zTcCbqjoaeNN9BjgfGO1es4EHwEtSwC3AycAk4JamRBUzgTQCNFJfH4zpZIwxpqdJRPXadGCO654DzAgrf0I9C4FcERkEnAfMU9VyVd0DzAOmxTRCfwoAwYa6mE7GGGN6mlgnHQVeF5HFIjLblQ1Q1W0A7r2/Kx8CbA77bqkra6n8ECIyW0SKRKTosJ+R7k8FoNGSjjHGRFUgxuM/RVW3ikh/YJ6IrGplWIlQpq2UH1qg+hDwEEBhYeFn+neIPw2wMx1jjIm2mJ7pqOpW974TeAHvmswOV22Ge9/pBi8FhoV9fSiwtZXy2HHVayFLOsYYE1UxSzoikiUiOU3dwLnAMuAloKkF2izgRdf9EvBV14ptMrDPVb+9BpwrInmuAcG5rix2At6ZTihoSccYY6IpltVrA4AXRKRpOk+p6qsisgh4TkSuAjYBX3LDvwxcAJQA1cDXAVS1XER+Bixyw92uquUxjPvANR1LOsYYE10xSzqqug44PkL5buCsCOUKXNvCuB4FHo12jC1y1WsarI/bJI0xpiewOxJE4hoSqJ3pGGNMVFnSieTAmU5DggMxxpjuxZJOJK4hgYTqCYUOr/W1McaYgyzpROIaEqQSpL7R7jRtjDHRYkknEle9Zo+sNsaY6LKkE0kgA4AM6uzpocYYE0WWdCLJyAWgl1RTZ8/UMcaYqLGkE0m6l3R6U2lnOsYYE0WWdCJJSafRn0GuVNk1HWOMiSJLOi0IpvUml0pLOsYYE0WWdFrQmNabXKmkLmjVa8YYEy2WdFqg6Xn0tuo1Y4yJKks6LdCMPHKppLreko4xxkSLJZ0WpOf0pbdUsWVvTaJDMcaYbsOSTgsCWX3IlSo27KpKdCjGGNNtWNJpSUYe6dSzdfeeREdijDHdhiWdlmTkAbBv944EB2KMMd2HJZ2WuKTTUFlOjTUmMMaYqLCk0xKXdHKpYlN5dYKDMcaY7sGSTktc0ukj+yneWZHgYIwxpnuwpNOSvkeiab24OP0jHliwlkZ7gqgxxhw2SzotSc1EJlzB2boQti3lykcW8sLHpeytrk90ZMYYk7QCsZ6AiPiBImCLqn5BREYCzwB9gI+AK1W1XkTSgCeAE4HdwGWqusGN48fAVUAjcJ2qvhbruAGY9F/IR3P4Z9pP2Lx1EK8+fwLf0kJ8w0/mzLGDOWfsAEb0zYpLKMYY0x2IamyrjUTku0Ah0MslneeAv6nqMyLyIPCJqj4gItcA41X1WyIyE7hYVS8TkbHA08AkYDDwBnCUqrbYpKywsFCLioqiMwP7t8Lql9FVr6Dr38EXqme/5DAveDxvNJ7I1n5TOGXsSM4eO4AJQ3Px+SQ60zXGmDgTkcWqWhjTacQy6YjIUGAOcCfwXeCLQBkwUFWDIjIFuFVVzxOR11z3+yISALYD+cBNAKr6CzfOA8O1NN2oJp1wdRWw9i1Y9TKNa17DX7uHBlJ4PzSG1xtP5KP0KYwfO4azxwxg6uh+pKf4ox+DMcbESDySTqyr1+4FfgjkuM99gb2qGnSfS4EhrnsIsBnAJaR9bvghwMKwcYZ/5wARmQ3MBhg+fHh056JJWg6MnQ5jp+NvDMLmD0hZ/TKnrPwnp+19DBofY8XSkTz70Wn8JOVszpswissnDWfs4F6xiccYY5JMzJKOiHwB2Kmqi0Xk9KbiCINqG/1a+87BAtWHgIfAO9PpcMAd5Q9AwSlQcAr+c++AXcWw+mWOWfEit22dw0/kWV77qJD//XAK1UNP46tTRzPt2IH4rfrNGNODxfJM5xTgIhG5AEgHeuGd+eSKSMCd7QwFtrrhS4FhQKmrXusNlIeVNwn/TtcgAvlHQf5R+KbeABvfJ+2Tp/nCihe5qPZdtpXN4dfPXsyvXjmXb3x+NF86cahVvRljeqSYNZlW1R+r6lBVLQBmAm+p6hXAfOBSN9gs4EXX/ZL7jOv/lnoXnF4CZopImmv5Nhr4MFZxR8WIKXDRffi+XwyXPcnAgYP5Vcof+XP99Sx66SFO/cU8HvnXOoKN9lRSY0zPkoj/6fwI+K6IlOBds/mTK/8T0NeVf5eDDQiWA88BK4BXgWtba7nWpQRSYcwXkNkL4LK/MKRPDvel/p4Xfd/j41ce45I/vMfq7Xa3A2NMzxHzJtOJELPWa4crFIKVL6Fv343sXMEijuXmhllMO+MMrj79CFID9l9dY0zixKP1mh3l4snng3EzkG+9Cxfew4nppfwj5SZY8HMu/v2/WFdWmegIjTEmpizpJILPDyddhe87i/GPv5TrA3/jhr0/5xv3v8zijfbQOGNM92VJJ5Gy+sHFf4Szb+Ns32KekZ9w3SOv827xrkRHZowxMWFJJ9FEYOoNyNdfYYC/godT7+H6J97h09J9iY7MGGOizpJOVzHsJOQ/HmZMqJjHA3dx/Zy32b6vNtFRGWNMVFnS6UrGXoR86XGOlbX8sv5OvvXEB9QFk6N1uDHGtEe7ko6IHOEePYCInC4i14lIbmxD66HGXoRMv59CWcVJ25/h5/9cmeiIjDEmatp7pvM80CgiR+L9iXMk8FTMourpxl8GR1/ID1Pn8s7ChSzeWJ7oiIwxJiram3RC7l5pFwP3quqNwKDYhdXDicAX7iGQlsG96Q9z8wufErLHZRtjuoH2Jp0GEbkc795o/3BlKbEJyQCQMxCZdhfH6yomlv2N15ZvT3RExhhz2NqbdL4OTAHuVNX17sabf4ldWAaA4y9HR53JTSnP8qd5H9nZjjEm6bUr6ajqClW9TlWfFpE8IEdV74pxbEYEOe8Osqjh1PK/2tmOMSbptbf12gIR6SUifYBPgMdE5J7YhmYAGDAOPeaLXBV4jUfmfUx3vEGrMabnaG/1Wm9V3Q/8B/CYqp4InB3rnc0GAAAbpklEQVS7sEw4Of1HZFPN1N1zWbjOWrIZY5JXe5NOQEQGAV/mYEMCEy8Dj6PxqAu5KuVVnn1/TaKjMcaYTmtv0rkdeA1Yq6qLRGQUUBy7sExz/smz6UUVoVUvs7e6PtHhGGNMp7S3IcFfVXW8ql7tPq9T1UtiG5o5RMFp1GcNZoa8wxsrdyY6GmOM6ZT2NiQYKiIviMhOEdkhIs+LyNBYB2fC+HykTLiM0/xLefcTuzWOMSY5tbd67THgJWAwMAT4uyszcSTH/gcBQmStf53KumCiwzHGmA5rb9LJV9XHVDXoXo8D+TGMy0Qy8Dhqc0ZwLgt5a5VVsRljkk97k84uEfmKiPjd6yvA7lgGZiIQIfW4iznFv5x/fWKt2Iwxyae9SecbeM2ltwPbgEvxbo1j4sw3bjoBGgmUvEptgz1rxxiTXNrbem2Tql6kqvmq2l9VZ+D9UbRFIpIuIh+KyCcislxEbnPlI0XkAxEpFpFnRSTVlae5zyWuf0HYuH7syleLyHmdntvuYPAJ1GQN5WxdyOKNexIdjTHGdMjhPDn0u230rwPOVNXjgQnANBGZDNwN/EZVRwN7gKvc8FcBe1T1SOA3bjhEZCwwExgHTAP+ICL+w4g7uYkQGDedU31LWbhqU6KjMcaYDjmcpCOt9VRPpfuY4l4KnAnMdeVzgBmue7r7jOt/loiIK39GVetUdT1QAkw6jLiTXspRZ5IqjZSvfi/RoRhjTIccTtJp886TrtHBEmAnMA9YC+x1D4QDKMVrgo173wzg+u8D+oaXR/hO+LRmi0iRiBSVlZV1bo6SxbCTCeFnwJ4idlfWJToaY4xpt1aTjohUiMj+CK8KvP/stEpVG1V1AjAU7+xkTKTBmibXQr+WyptP6yFVLVTVwvz8bt6aOy2H2n7HcrJvFe+W7Ep0NMYY026tJh1VzVHVXhFeOaoaaO9EVHUvsACYDOSKSNN3hwJbXXcpMAzA9e8NlIeXR/hOj5U++jQm+NaycPWWRIdijDHtdjjVa60SkXwRyXXdGXiPQlgJzMdrcg3e469fdN0vuc+4/m+p9/CYl4CZrnXbSGA08GGs4k4WvoJTSKOBPcUL7Rk7xpik0e6zlU4YBMxxLc18wHOq+g8RWQE8IyJ3AB8Df3LD/wn4s4iU4J3hzARQ1eUi8hywAggC16qq/UFl+GQU4ciapazfVcWo/OxER2SMMW2KWdJR1aXACRHK1xGh9Zmq1gJfamFcdwJ3RjvGpJaRR33fMZy8cyUfri+3pGOMSQoxq14zsZd6xFQK/cUUrbP7sBljkoMlnSQmI04hgzr2r1uU6FCMMaZdLOkksxGnADCy6hNK91QnOBhjjGmbJZ1klp1PXe6RnOxbyaIN5YmOxhhj2mRJJ8mljprKSf7VLFrXze/CYIzpFizpJDkZeSo51LBnbVGiQzHGmDZZ0kl2Iz/vve37kJ0VtQkOxhhjWmdJJ9ll51PdZyyn+paxaL09X8cY07VZ0ukG0o4+k0Lfaj4uKU10KMYY0ypLOt2Af+TnSZFG9q/9INGhGGNMqyzpdAfDTgJgwL4l7K2uT3AwxhjTMks63UFGHjW5R3GirGHhut2JjsYYY1pkSaebSB01hYm+Yt4rtvuwGWO6Lks63YR/+BR6STVb1nyc6FCMMaZFlnS6i+EnAzBo/yds2VuT4GCMMSYySzrdRd5Ighn5nOhbw3sluxIdjTHGRGRJp7sQwV8wmUl+SzrGmK7Lkk43IsMmM5SdrCleg6omOhxjjPkMSzrdySjvPmzH1haxZkdlgoMxxpjPsqTTnQw4lsasgXze9wnvWhWbMaYLsqTTnYjgP+ocPu9fxvvFOxIdjTHGfEbMko6IDBOR+SKyUkSWi8j1rryPiMwTkWL3nufKRUTuE5ESEVkqIhPDxjXLDV8sIrNiFXO3cOTZ5FBF1foPaWgMJToaY4w5RCzPdILA91R1DDAZuFZExgI3AW+q6mjgTfcZ4HxgtHvNBh4AL0kBtwAnA5OAW5oSlYlgxOcAOK5xBR9ttEcdGGO6lpglHVXdpqofue4KYCUwBJgOzHGDzQFmuO7pwBPqWQjkisgg4DxgnqqWq+oeYB4wLVZxJ73s/jT2OZLJ/tXMX22PsDbGdC1xuaYjIgXACcAHwABV3QZeYgL6u8GGAJvDvlbqyloqNy3wF3yOSf41LFi5PdGhGGPMIWKedEQkG3geuEFV97c2aIQybaW8+XRmi0iRiBSVlfXwX/gjTiFbK/GXLbdb4hhjupSYJh0RScFLOE+q6t9c8Q5XbYZ7b7otcikwLOzrQ4GtrZQfQlUfUtVCVS3Mz8+P7owkm1FnAHC6bwnzV9ldp40xXUcsW68J8CdgpareE9brJaCpBdos4MWw8q+6VmyTgX2u+u014FwRyXMNCM51ZaYlOQPQwRM5P20JC1Zb0jHGdB2xPNM5BbgSOFNElrjXBcBdwDkiUgyc4z4DvAysA0qAh4FrAFS1HPgZsMi9bndlphVy1HmMDRWzomQ9tQ2NiQ7HGGMACMRqxKr6LpGvxwCcFWF4Ba5tYVyPAo9GL7oe4Kjz8C34BZMbF/Neyec5a8yAREdkjDF2R4Jua+DxaPYAzkv9hH8u3ZboaIwxBrCk0335fMjocznNt5S3VmyxKjZjTJdgSac7O2oaGaEqxjYs4501PbwZuTGmS7Ck050dcSaaksnFqYv4h1WxGWO6AEs63VlqJjL6XKYFinhr5TarYjPGJJwlne5u7HRygnsY17CC15bbbXGMMYllSae7G30uGkjny1mLeeqDTYmOxhjTw1nS6e7SspHR5zDN9yEfrt9Fyc6KREdkjOnBLOn0BGNnkFW/i88FVvOkne0YYxLIkk5PcPQFkNabG/Pe4/nFpdagwBiTMJZ0eoLUTJhwOROr/kVK7W7mLi5NdETGmB7Kkk5PUfgNfKEGruv7IQ++vZaGxlCiIzLG9ECWdHqK/KNhxFS+zDy27KnihY+3JDoiY0wPZEmnJznpG2RUlXJlv7X8YX4JQTvbMcbEmSWdnuSYL0JmP67t9Q4bdlfbrXGMMXFnSacnCaTCxCvpv20+U/Nr+f38EkIhTXRUxpgexJJOT3Pi1xHgloHvUbKzkr8v3ZroiIwxPYglnZ4mbwSMnc6Rm/7KiQMD/O9rq+1/O8aYuLGk0xN97jtI3X5+PeIDSvfUMOffGxIdkTGmh7Ck0xMNORGOOp+C1Y9w4ZHp/H5+CeVV9YmOyhjTA1jS6anO/CnUVfCLzKeorm/kvjeLEx2RMaYHsKTTUw08Fk77Ib3WzOW2ozbwl4UbWVtWmeiojDHdXMySjog8KiI7RWRZWFkfEZknIsXuPc+Vi4jcJyIlIrJURCaGfWeWG75YRGbFKt4e6bQfQL+jmLn/MbJShR8//6k1oTbGxFQsz3QeB6Y1K7sJeFNVRwNvus8A5wOj3Ws28AB4SQq4BTgZmATc0pSoTBT4A3DmTwmUr+HR41bw4YZynnh/Q6KjMsZ0YzFLOqr6DlDerHg6MMd1zwFmhJU/oZ6FQK6IDALOA+aparmq7gHm8dlEZg7HmIug4FQmrrmXGUcId7+6mo27qxIdlTGmm4r3NZ0BqroNwL33d+VDgM1hw5W6spbKTbSIwBd/izQ2cLf/AVJ8yg/nLrVqNmNMTHSVhgQSoUxbKf/sCERmi0iRiBSVlZVFNbhur+8RMO0XpG16h7+M+YAP1pfz5AcbEx2VMaYbinfS2eGqzXDvO115KTAsbLihwNZWyj9DVR9S1UJVLczPz4964N3eiV+DsTM4bs3v+K/h2/jFK6vYtLs60VEZY7qZeCedl4CmFmizgBfDyr/qWrFNBva56rfXgHNFJM81IDjXlZloa6pmyxvJj/f8D8fLWr799Ed2ixxjTFTFssn008D7wNEiUioiVwF3AeeISDFwjvsM8DKwDigBHgauAVDVcuBnwCL3ut2VmVjIyIWv/RNfRh8eyp3D8tJybn1peaKjMsZ0I4FYjVhVL2+h11kRhlXg2hbG8yjwaBRDM63JGQDn3UnOX2cxZ/S7fGWRn3GDe3HllIJER2aM6Qa6SkMC05WMnQ7HfYmpm//IK7n/y/N/f4lXl21PdFTGmG7Ako75LBGY/geY8m2O8ZVyf8aD/PmZJylatT7RkRljkpwlHRNZIBXOuxOZcT9DGrfwZOB2dj5zLcu37kt0ZMaYJGZJx7TuqPPgwl9TM+o8zuN9vvvHl5i/amfb3zPGmAgs6Zi2nfRNMmbci88f4N7A77hpzus88q91eO0/jDGm/SzpmPbpNRi55BGOkU28mfFjlr/yENc+uZgte2sSHZkxJolY0jHtN3Y68v/eJmvwMfwm9QG+WXw1d/76V9z7+kpq6u1PpMaYtkl3rCIpLCzUoqKiRIfRfYUaYfHjBN+5h0BFKfXq5x/+s0m56Nd84fihiES6ZZ4xpqsTkcWqWhjTaVjSMZ3WGIRVf2fXRy/Rb+3fqFc/a1KOZtWUX3Pm5EL6ZKUmOkJjTAdY0ukkSzpxpkqo6DHWffo+gzf9Hb828GpoEutyp5BxzNmcdOwYJgzLxe+zMyBjujJLOp1kSSeB9m6i/PVfkl78TzIbvNvk7dRcNskg1vc5ldDQkxk+aCBHjiogf8DQBAdrjAlnSaeTLOl0AaEQbF9Kzeq3KNuwDP+OTxhSW3Kgd1B9FMmx5Pv2k0MVRf0uJjU7DwaOpzYYYsrUs+jbKyuBM2BMz2NJp5Ms6XRR+0qp3fwJW8p2U7V+Ef12vk+d+qGxgZHBtYcMWqEZlMoA6vy92J0xgsy0FIYGN7Ol4GKyM9PJTw+xmzyOyK4j7ZhpkNU3QTNlTPdhSaeTLOkkmVAIasqp2LOTii2r0IYaylcsILWylEDdHgbWbyRV6ynTXgyWyE+2qJIsqv057JccatLyCeUMISfNx9Bt8ygfMIX6vmMI9D+a7KxMsnr1xddrAGTlg4YgJQv8MbvhujFJw5JOJ1nS6WZUobGeukZl77rF7KwLsHlfkAHsYeWuBgaVvUv9/p2kBivIo4KchjLyQ7vIpob3Qsdyom8N2VLb4uhDCOUpA9mTOZJgr+H0DgTx5/RHB59AZgAys7II9D8G9m4iWDQH8kYQOOMmWPQI+FPhpP8Cn/3lzSQ/SzqdZEnH1NQ3snVvFTsq6kkP+KipriC4fQX7axqor9yDVuzAX11GZb0i9fsZGNzCkOBmhrCTOlLJpZIU+ewfXqs0jSypI4iPACEA6iUNQdmXNpiKvDE0pOYhPh/1mQPwiY+UUDVZUk/u+PNJzx2E9DsKGqqhcgfkDodAWrwXjzERWdLpJEs6pjNUlbLKOkr31LBvfwXsXE5lg1BdVUXq3mIqpDf7Bp3CqPJ3SClbRpGMI7O+nOH1xdQ2+shv2MJYWU8vqvETIlPqAGhUIYifNAl+ZpohfJQFBlGV0odMaqhM609QA+TXb2J73omE0vtQ26uAlECAQHomgbQsMoP7yN5fguYfQ6DPCFJ79SO9V38kqx+EglBaBP3HeE+CbUvVLkjNhpT0aC9Ok4TikXSsItsYR0Ton5NO/5x0IA8Y3sKQ4wE4t1lpQ2OIvdUNVIZCBBuCbK+rJBgSgo0hduypYP+Gxfgrd5BVtYmaRh/b6UOvmlL6120ip7acTdqb/Oqt5FDDSs3n+OqXyaCOgIQ+E0FIBZ8c+oOxinQEyKSWKjKo8PUinTr2+fuwM60ADaST4QuSSgOVaQNJ0XrG7vg7VdkjWD/hB+TsWUEorRdZdTsIpKZTN/IceqX7Sfcpmj2AtOxcqK+C3kNBfAeWmTEdYWc6xnRBqkp1fSNV1TU0lG+gtiFEQ00l9bVVVEsWu9OHEChfi6+6DKrL8VeXkVlVSmOokfWpR1NQuQRfqI5qTSU3WMaQhk34NUitplCPn6GUoQhvh45nqu/TQ655BdWHD/1MUmtSqynskjxqNZWQP406UtFAGg2BHIL4GdKwgUCojv2BfhTUF7Ml7Qi29zuZYEY+6b5GJDWLxpQs9ofSGRLYT/8d/6K6/wRqB55Eem0ZoT5HkBKswpedjy93MKmhOvxZfUit3Y2veicN//odweO+TM7Y8/BF+sNxKNS+a2w7VwIC/Y/p5Frqfqx6rZMs6RjTOg010hiChhDs2LaJYFkx9b1GoNV7qCaT+toq/HvXsa9WqQ8JvWs2EaqvptaXSXblRrIadpFGA6GGWgJaDw21ZDRWkEY9mwIjSKGRvsEdLAyNYZxsYEyopMUkVqa9yZfWHw7YqII/7PshFXaSSy3pBKSRDOpJpY506kmhkU0ymD2SR62kka1V+AhRTQbljWlUaiZDUqsoDC7Gh/JRxhT2pA4iR6uoC2ShoRC9G8qokgy2pRSQmepHRGlKbz5CBEL1NPpSKcs4goKaZQyrXkHxwAuoyh6JhhqRUAMpBPH5fPh9Qv7+FWzLnUi1L4ec6o2M3v4yJUMvpjoo1OaMIC0zh/TUAKmBAP6M3mTU7yJr3avs7TOe6iAMqlyBLz2HupwCGnILkOz+pPoAEUIIwWADPl8Av89HbmYKw/pkdmq7sKTTSZZ0jOlaQvW11FbtoabRT111BaG6CrKoZWdFPftyxyEVW0gvW0p1Wn/S9m+gJtALX81u0qp3UO9LI7WmjIqUfIL4qRp2GkM2voi/Ygu+xlqC6qNO0lB/Og2+VOpCfgZVryY1VENaqIZafxYNkkpGYxXZVJHeWEUFGRSlnkQIP5+vmUdmqIq90oteWoEibPf1p4/uI0/3RpyfIO5sECWkwhbtxzBfWbuXR50GIl7ja69KTSedegRlP1nkUE0lGezRbDb2+Ryn3/B4p8Zr13SMMd2CLzWdzNRBeL+/+x8ozzvQ1Zema2Xt87nDiqcXMKTpg/vhPVDkQPcRTd0NNSACSNi7j4A/ALX7oHwdvsy+DMkZQt32FTTs34E/kIL4UwmKn1BjiMba/dT1G0vGjiX4acSXmkn9wBOpXzuP1NxBBMs3UltTTbAxRGNjEK3dR50vg8bhp5K1s4hAdj67co8jWFdN2r4NpOxbR8r+jdT7MkGEtIZ97E/tRaBuD+kNVRw55ITDWjaxljRnOiIyDfgt4AceUdW7WhrWznSMMabj4nGmkxT/aBMRP3A/cD4wFrhcRMYmNipjjDEdlRRJB5gElKjqOlWtB54Bpic4JmOMMR2ULElnCLA57HMpYVWyACIyW0SKRKSorKz9F/SMMcbET7IknUj/QDvkYpSqPqSqhapamJ+fH6ewjDHGdESyJJ1SYFjY56HA1gTFYowxppOSJeksAkaLyEgRSQVmAi8lOCZjjDEdlBT/01HVoIh8G3gNr8n0o6q6PMFhGWOM6aCkSDoAqvoy8HKi4zDGGNN5SfPn0I4QkTJg42GMoh+wK0rhJFJ3mQ+weemqbF66ps7OywhVjWlLrG6ZdA6XiBTF+l+58dBd5gNsXroqm5euqSvPS7I0JDDGGNMNWNIxxhgTN5Z0Inso0QFESXeZD7B56apsXrqmLjsvdk3HGGNM3NiZjjHGmLixpBNGRKaJyGoRKRGRmxIdT0eJyAYR+VRElohIkSvrIyLzRKTYvee1NZ5EEJFHRWSniCwLK4sYu3juc+tpqYhMTFzkn9XCvNwqIlvculkiIheE9fuxm5fVInJeYqL+LBEZJiLzRWSliCwXketdedKtl1bmJRnXS7qIfCgin7h5uc2VjxSRD9x6edbdvQURSXOfS1z/gkTGj6ray6ti9ANrgVFAKvAJMDbRcXVwHjYA/ZqV/RK4yXXfBNyd6DhbiP00YCKwrK3YgQuAV/BuBDsZ+CDR8bdjXm4Fvh9h2LFuW0sDRrpt0J/oeXCxDQImuu4cYI2LN+nWSyvzkozrRYBs150CfOCW93PATFf+IHC1674GeNB1zwSeTWT8dqZzUHd9Zs90YI7rngPMSGAsLVLVd4DyZsUtxT4deEI9C4FcERkUn0jb1sK8tGQ68Iyq1qnqeqAEb1tMOFXdpqofue4KYCXeI0WSbr20Mi8t6crrRVW10n1McS8FzgTmuvLm66Vpfc0FzhKRSHfujwtLOge1+cyeJKDA6yKyWERmu7IBqroNvB2P8AfUd30txZ6s6+rbrtrp0bBqzqSYF1clcwLer+qkXi/N5gWScL2IiF9ElgA7gXl4Z2J7VTXoBgmP98C8uP77gL7xjfggSzoHtfnMniRwiqpOxHus97UiclqiA4qRZFxXDwBHABOAbcCvXXmXnxcRyQaeB25Q1f2tDRqhrKvPS1KuF1VtVNUJeI95mQSMiTSYe+9S82JJ56Ckf2aPqm517zuBF/A2xh1NVRzufWfiIuywlmJPunWlqjvcgSIEPMzBqpouPS8ikoJ3kH5SVf/mipNyvUSal2RdL01UdS+wAO+aTq6INN3EOTzeA/Pi+vem/dW/UWdJ56CkfmaPiGSJSE5TN3AusAxvHma5wWYBLyYmwk5pKfaXgK+61lKTgX1N1T1dVbNrGxfjrRvw5mWma2E0EhgNfBjv+CJx9f5/Alaq6j1hvZJuvbQ0L0m6XvJFJNd1ZwBn412jmg9c6gZrvl6a1telwFvqWhUkRKJbYnSlF17rmzV49aP/neh4Ohj7KLzWNp8Ay5vix6u7fRModu99Eh1rC/E/jVe90YD3y+yqlmLHqy64362nT4HCRMffjnn5s4t1Kd5BYFDY8P/t5mU1cH6i4w+LaypeNcxSYIl7XZCM66WVeUnG9TIe+NjFvAy42ZWPwkuMJcBfgTRXnu4+l7j+oxIZv92RwBhjTNxY9Zoxxpi4saRjjDEmbizpGGOMiRtLOsYYY+LGko4xxpi4saRjTAeISGPYHYmXSBTvRi4iBeF3pjamOwq0PYgxJkyNercfMcZ0gp3pGBMF4j3L6G73nJMPReRIVz5CRN50N5R8U0SGu/IBIvKCeybKJyLyOTcqv4g87J6T8rr7xzkicp2IrHDjeSZBs2nMYbOkY0zHZDSrXrssrN9+VZ0E/B6415X9Hu92/+OBJ4H7XPl9wNuqejzes3eWu/LRwP2qOg7YC1ziym8CTnDj+VasZs6YWLM7EhjTASJSqarZEco3AGeq6jp3Y8ntqtpXRHbh3VqlwZVvU9V+IlIGDFXVurBxFADzVHW0+/wjIEVV7xCRV4FK4P+A/9ODz1MxJqnYmY4x0aMtdLc0TCR1Yd2NHLzueiHefc1OBBaH3U3YmKRiSceY6Lks7P191/1vvDuWA1wBvOu63wSuhgMP5OrV0khFxAcMU9X5wA+BXOAzZ1vGJAP7tWRMx2S4JzY2eVVVm5pNp4nIB3g/5i53ZdcBj4rID4Ay4Ouu/HrgIRG5Cu+M5mq8O1NH4gf+IiK98e7k/Bv1nqNiTNKxazrGRIG7plOoqrsSHYsxXZlVrxljjIkbO9MxxhgTN3amY4wxJm4s6RhjjIkbSzrGGGPixpKOMcaYuLGkY4wxJm4s6RhjjImb/w8AtwO4RWXHwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Cost for Neural Network Simulating Credit Approval Application')\n",
    "plt.plot(history.history['loss'],label='train')\n",
    "plt.plot(history.history['val_loss'],label='validate')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('cont.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4062/4062 [==============================] - 0s 17us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "155.87774464369173"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_valid[:100]) > 0.5\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36,  0],\n",
       "       [ 0, 64]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_valid[:100], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False]], dtype=bool)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_approval_bug(citizen,state,age,sex,region,income_class,no_depend,marital):\n",
    "    approved = 0\n",
    "    amount = 0\n",
    "    if (region == 5 or region == 6):\n",
    "        amount = 0\n",
    "    else:\n",
    "        if (age < 18): ### BUG\n",
    "            amount = 0\n",
    "        else:\n",
    "            if (citizen == 0):\n",
    "                amount = 5000 + 1000*income_class\n",
    "                if state == 0:\n",
    "                    if (region == 3 or region == 4):\n",
    "                        amount = amount*2.00\n",
    "                    else:\n",
    "                        amount = amount*1.50\n",
    "                else:\n",
    "                    amount = amount*1.10\n",
    "                if (marital == 0):\n",
    "                    if (no_depend > 0):\n",
    "                        amount += 200*no_depend\n",
    "                    else:\n",
    "                        amount += 500\n",
    "                else:\n",
    "                    amount += 1000\n",
    "                if (sex == 1): ##BUG\n",
    "                    amount += 500\n",
    "                else:\n",
    "                    amount += 1000\n",
    "            else:\n",
    "                amount = 1000 + 800*income_class\n",
    "                if (marital == 0):\n",
    "                    if (no_depend > 2):\n",
    "                        amount += 100*no_depend\n",
    "                    else:\n",
    "                        amount += 100\n",
    "                else:\n",
    "                    amount += 300\n",
    "                if (sex == 1): #BUG\n",
    "                    amount += 100\n",
    "                else:\n",
    "                    amount += 200\n",
    "    if amount == 0:\n",
    "        approved = 0\n",
    "    else:\n",
    "        approved = 1\n",
    "    return approved, amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
